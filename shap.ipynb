{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shap används ungefär som permutation importance för att hitta bästa features för en LSTM med tidsseriedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_fn(x):\n",
    "    # Convert the input to tensor if it's not and ensure it has the right shape\n",
    "    if not isinstance(x, torch.Tensor):\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "    \n",
    "    # Ensure x is 3D: [batch_size, sequence_length, features]\n",
    "    if x.dim() == 2:\n",
    "        x = x.unsqueeze(1)  # Adds a sequence length of 1\n",
    "    \n",
    "    # Move the tensor to the correct device\n",
    "    x = x.to(device)\n",
    "    \n",
    "    # Ensure model is in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        predictions = model(x)\n",
    "        # Convert predictions to probabilities if it's a classification model\n",
    "        predictions = torch.softmax(predictions, dim=1).cpu().numpy()\n",
    "    \n",
    "    #return predictions.cpu().numpy()\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# Sample a subset of your validation dataset and ensure it's 2D\n",
    "# Here, `shap.sample()` is replaced with manual sampling and reshaping for clarity\n",
    "num_samples = 100  # Number of samples to use for background distribution\n",
    "selected_time_step = 0  # Example: choose the first time step for simplicity\n",
    "\n",
    "# Ensure X_val_np is a numpy array and select a specific time step for all samples\n",
    "X_val_sampled = X_val_np[:num_samples, selected_time_step, :]  # This selects the first time step for the first `num_samples`\n",
    "\n",
    "# Now, ensure that explainer is created with 2D data\n",
    "explainer = shap.KernelExplainer(predict_fn, X_val_sampled)\n",
    "\n",
    "# Similarly, when computing SHAP values, ensure the input data is 2D\n",
    "X_val_to_explain = X_val_np[:10, selected_time_step, :]  # Explain the first 10 samples at the selected time step\n",
    "shap_values = explainer.shap_values(X_val_to_explain)\n",
    "\n",
    "\n",
    "shap.initjs()\n",
    "# Visualization\n",
    "shap.force_plot(\n",
    "    base_value=explainer.expected_value[0],  # Base value or expected value of the model\n",
    "    shap_values=shap_values[0],  # SHAP values for a single instance or batch\n",
    "    features=X_val_to_explain  # Features corresponding to shap_values\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.interaction_plot(feature_index, shap_values, features)\n",
    "\n",
    "# If shap_values is a list of lists with a single inner list (for a single-output model),\n",
    "# extract the inner list:\n",
    "if isinstance(shap_values, list) and len(shap_values) == 1:\n",
    "    shap_values = shap_values[0]\n",
    "\n",
    "# Now try creating the beeswarm plot again\n",
    "shap.summary_plot(shap_values, X_val_np[0], plot_type=\"beeswarm\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
